import streamlit as st
from streamlit_agraph import agraph, Node, Edge, Config
from langchain_openai import OpenAI
from langchain.prompts import PromptTemplate
import json

# Definir o layout da p√°gina como centralizado
st.set_page_config(layout="wide", 
                   page_title="Mapas Mentais ü§ñ", page_icon="üß†")

# Fun√ß√£o para inicializar vari√°veis de estado
if 'nodes_data' not in st.session_state:
    st.session_state['nodes_data'] = None
if 'edges_data' not in st.session_state:
    st.session_state['edges_data'] = None
if 'response' not in st.session_state:
    st.session_state['response'] = None

# Estilos CSS para melhorar o design
st.markdown("""
    <style>
    .stButton button {
        background-color: #4CAF50;
        color: white;
        border-radius: 12px;
        padding: 10px 24px;
        font-size: 16px;
    }
    .stTextInput input {
        border-radius: 12px;
        padding: 10px;
    }
    .stTextInput label {
        font-weight: bold;
    }
    </style>
""", unsafe_allow_html=True)

# T√≠tulo da p√°gina
st.title("Mapas Mentais ü§ñüß†")

# Espa√ßamento para centralizar os elementos
st.header("")

SideL, SideR = st.columns((1, 1))

with st.sidebar:
    api_key = st.text_input("Insira sua OpenAI API key:", type="password")

with SideL:
    # Criar uma barra de pesquisa e um bot√£o de pesquisa
    MarginL, col1, col2m, MarginR = st.columns((0.5, 2, 1, 0.5))
    with col1:
        search_query = st.text_input("Qual o tema principal ‚ùì", "")
    with col2m:
        search_button = st.button("Pesquisar üîé")

# A√ß√£o ao clicar no bot√£o de pesquisa
if search_button:
    if not api_key:
        st.error("Por favor, insira sua OpenAI API key.")
    elif search_query:
        st.write(f"Voc√™ pesquisou por: **{search_query}**")

        # Configurar o LLM da OpenAI com a chave da API
        llm = OpenAI(openai_api_key=api_key, temperature=0, max_tokens=1500)

        # Criar um prompt template
        prompt_template = """
        Voc√™ √© um especialista em cria√ß√£o de mapas mentais. Gere um mapa mental estruturado e detalhado para o t√≥pico "{topic}".
        1. Para cada conceito, inclua seu nome, import√¢ncia (como n√∫mero de 1 a 5) e uma breve explica√ß√£o sobre sua fun√ß√£o.
        2. Relacione os conceitos de forma que o mapa mental seja coeso e reflita a interdepend√™ncia dos elementos.
        3. As rela√ß√µes entre os conceitos devem ser claramente nomeadas, indicando o tipo de conex√£o (ex: "causa", "depende de", "√© parte de").
        4. O mapa mental deve seguir uma hierarquia l√≥gica, com os conceitos mais amplos no topo e seus subconceitos de forma hier√°rquica abaixo.
        5. A sa√≠da deve estar no formato JSON e conter duas chaves: "nodes" e "edges".
        - "nodes" deve ser uma lista de dicion√°rios contendo "id", "label", "importance", "description" e "level" (o n√≠vel hier√°rquico do conceito).
        - "edges" deve ser uma lista de dicion√°rios contendo "source", "target" e "label" (explicando a rela√ß√£o entre os n√≥s).
        6. O JSON deve estar completo, v√°lido e sem cortes ou quebras.
        7. N√£o inclua explica√ß√µes adicionais ou texto antes ou depois do JSON.
        """

        prompt = PromptTemplate(
            input_variables=["topic"],
            template=prompt_template,
        )

        final_prompt = prompt.format(topic=search_query)

        # Obter a resposta do LLM
        response = llm.invoke(final_prompt)

        # Tentar analisar a resposta como JSON
        try:
            # Remover espa√ßos em branco e caracteres extras
            response = response.strip()
            # Verificar se a resposta come√ßa e termina com chaves
            if not response.startswith('{') or not response.endswith('}'):
                raise ValueError("A resposta n√£o √© um JSON v√°lido.")

            data = json.loads(response)
            st.session_state['nodes_data'] = data["nodes"]
            st.session_state['edges_data'] = data["edges"]
            st.session_state['response'] = response

        except Exception as e:
            st.error(f"Erro ao analisar a resposta: {e}")
            st.text("Resposta do LLM:")
            st.text(response)

with SideR:
    # Exibir o gr√°fico se os dados estiverem dispon√≠veis
    if st.session_state['nodes_data'] and st.session_state['edges_data']:
        # Definir cores e tamanhos dos n√≥s baseados na import√¢ncia e n√≠vel
        def get_node_size(importance):
            return 15 + importance * 5  # Tamanho baseado na import√¢ncia

        def get_node_color(level):
            colors = ['#FFD700', '#FF8C00', '#FF4500', '#FF6347', '#8B0000']  # Cores de acordo com o n√≠vel
            return colors[level % len(colors)]  # Seleciona cor baseado no n√≠vel hier√°rquico

        node_objects = [
            Node(
                id=str(node["id"]),
                label=node["label"],
                size=get_node_size(node.get("importance", 1)),
                color=get_node_color(node.get("level", 0)),
                title=node["description"]
            )
            for node in st.session_state['nodes_data']
        ]

        edge_objects = [
            Edge(
                source=str(edge["source"]),
                target=str(edge["target"]),
                label=edge["label"]
            )
            for edge in st.session_state['edges_data']
        ]

        # Configura√ß√£o para o agraph
        config = Config(
            width=950, height=750, directed=True, 
            physics=True, hierarchical=False,
            nodeHighlightBehavior=True, 
            maxZoom=2, minZoom=0.5,
            link={'label': 'true'}  # Exibir labels nas arestas
        )

        # Exibir o gr√°fico
        return_value = agraph(nodes=node_objects, edges=edge_objects, config=config)

        # Exibir a resposta JSON bruta
        with st.expander("Ver JSON gerado"):
            st.json(st.session_state['response'])
